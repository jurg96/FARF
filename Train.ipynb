{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import utils\n",
    "from modules import RotationLSH, IdentityLSH, PcaLSH, RandomForestClustRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG\n",
    "\n",
    "data_dir = '../data/Set14'\n",
    "train_files = ['barbara.png', 'bridge.png', 'comic.png', 'pepper.png', 'man.png']\n",
    "\n",
    "n_samples=10000\n",
    "s=1/2\n",
    "n_estimators=10\n",
    "max_depth=15\n",
    "max_features=20\n",
    "\n",
    "augmented = True        # whether to use gradient magnitudes or not\n",
    "lsh_type = 'rotation'   # ['rotation', 'pca', 'identity']\n",
    "n_components = 50       # makes sense only for PcaLSH\n",
    "binary = False          # makes sense only for RotationLSH\n",
    "reg_type = 'lasso'      # ['ridge', 'lasso']\n",
    "\n",
    "min_samples_leaf = 64\n",
    "alpha = 4               # regularization parameter\n",
    "\n",
    "\n",
    "config = dict(\n",
    "    data_dir=data_dir,\n",
    "    train_files=train_files,\n",
    "    n_samples=n_samples,\n",
    "    s=s,\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    max_features=max_features,\n",
    "    augmented=augmented,\n",
    "    lsh_type=lsh_type,\n",
    "    n_components=n_components,\n",
    "    binary=binary,\n",
    "    reg_type=reg_type,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    alpha=alpha\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trainset(n_samples, s, seed=7):\n",
    "    # patch_size is (9,9)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for file in train_files:\n",
    "        path = os.path.join(data_dir, file)\n",
    "        img_h = utils.load_image(path)\n",
    "        img_h = utils.crop_image(img_h, s)\n",
    "        new_shape = img_h.shape\n",
    "        \n",
    "        img_l = utils.resize(utils.resize(img_h, s), 1/s) # downsample -> upsample\n",
    "\n",
    "        features = utils.extract_features(img_l, augmented=augmented)\n",
    "        img_out_delta = img_h - img_l\n",
    "        assert img_h.shape == img_l.shape == img_out_delta.shape == new_shape\n",
    "        \n",
    "        patches_per_picture = n_samples//len(train_files)\n",
    "        \n",
    "        x_ind = np.random.randint(0, new_shape[0]-9, size=patches_per_picture)\n",
    "        y_ind = np.random.randint(0, new_shape[1]-9, size=patches_per_picture)\n",
    "        for x, y in zip(x_ind, y_ind):\n",
    "            feat_patch = features[x:x+9, y:y+9]\n",
    "            delta_patch = img_out_delta[x:x+9, y:y+9]\n",
    "            X.append(feat_patch.flatten())\n",
    "            Y.append(delta_patch.flatten())\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lsh_type == 'identity':\n",
    "    lsh = IdentityLSH()\n",
    "elif lsh_type == 'rotation':\n",
    "    lsh = RotationLSH(binary=binary)\n",
    "elif lsh_type == 'pca':\n",
    "    lsh = PcaLSH(n_components=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_trainset(\n",
    "    n_samples=n_samples,\n",
    "    s=s,\n",
    "    seed=7\n",
    ")\n",
    "print('X.shape:\\t {}'.format(X.shape))\n",
    "print('Y.shape:\\t {}'.format(Y.shape))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "lsh.fit(X)\n",
    "X_comp = lsh.transform(X)\n",
    "print('X_comp.shape:\\t {}'.format(X_comp.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClustRegressor(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    n_jobs=None,\n",
    "    max_features=max_features,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    reg_type=reg_type,\n",
    "    alpha=alpha\n",
    ")\n",
    "rf.fit(X, X_comp, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_name = './trained_models/example_model.pkl'\n",
    "\n",
    "model = {\n",
    "    'scaler': scaler,\n",
    "    'lsh': lsh,\n",
    "    'rf': rf,\n",
    "    'config': config\n",
    "}\n",
    "with open(save_name, 'wb') as output:\n",
    "    pickle.dump(model, output, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main-env)",
   "language": "python",
   "name": "main-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
